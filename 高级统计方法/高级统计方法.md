# 高级统计方法（Statistical Learning）知识点整理

其实是机器学习，深度学习，统计学习相关的

统计学习

监督学习（指导学习）VS  无监督学习（无指导学习）

## 第1章-1 导论与统计基础复习

### 一、统计学习可解决的问题类型

1. **识别风险因子**

   - 例：前列腺癌风险因子识别
2. **预测问题**

   - 例：预测心脏病发病、垃圾邮件检测、手写邮政编码识别
3. **分类问题**

   - 例：基于基因表达谱的癌症分类
4. **关系建模**

   - 例：工资与人口统计学变量之间的关系建模

---

### 二、监督学习（Supervised Learning）

#### 基本概念：

- **Y**：结果变量（因变量、响应变量、目标变量）
- **X**：预测变量（自变量、输入、协变量、属性）
- **回归问题**：Y为连续定量变量（如价格、血压）
- **分类问题**：Y为有限无序类别（如生死、数字、癌症类型）
- **目标**：通过训练数据建立 X → Y 的预测模型

#### 监督学习的目标：

1. **预测**：准确预测未见过的测试实例
2. **推断**：理解哪些变量影响 Y，如何影响
3. **评估**：评价模型的预测和推断效果

---

### 三、无监督学习（Unsupervised Learning）

#### 特点：

- 无结果变量 Y，只有预测变量 X
- 无标签数据，无法进行传统分类
- 目标模糊：聚类、降维、推荐系统等

#### 典型应用：

- 聚类分析（Clustering）
- 特征提取与降维
- 推荐系统

#### 与监督学习的区别：

- 无监督学习常用于数据探索或预处理
- 难以评估模型表现

---

### 四、统计学习与机器学习的关系

| 方面       | 统计学习                   | 机器学习               |
| ---------- | -------------------------- | ---------------------- |
| 背景       | 统计学子领域               | 人工智能子领域         |
| 重点       | 模型解释性、精度、不确定性 | 大规模应用、预测准确性 |
| 方法       | 强调模型结构与推断         | 强调算法与计算效率     |
| 交叉性     | 两者交叉越来越多，界限模糊 |                        |
| 市场影响力 | 较弱                       | 较强                   |

---

### 五、统计学习的历史与代表人物

| 人物               | 贡献                                             |
| ------------------ | ------------------------------------------------ |
| 勒让德、高斯       | 最小二乘法、正态分布                             |
| 费舍尔             | 线性判别分析、逻辑回归、方差分析、Fisher精确检验 |
| 内尔德、韦德伯恩   | 广义线性模型                                     |
| 布赖曼等           | 分类回归树（CART）、交叉验证                     |
| 哈斯帖、提布施瓦尼 | 广义可加模型（GAM）                              |
| 托马斯·贝叶斯     | 贝叶斯定理                                       |
| 卡尔·皮尔逊       | 相关系数、卡方检验、P值                          |
| 威廉·戈塞         | t检验（小样本检验）                              |
| C.R.拉奥           | C-R不等式、Rao-Blackwell定理、微分几何与统计     |
| 弗罗伦斯·南丁格尔 | 南丁格尔玫瑰图、统计图形可视化先驱               |

---

### 六、课程教材与章节安排

#### 教材：

- **ISLR**（An Introduction to Statistical Learning）：适合入门，含R实验
- **ESL**（The Elements of Statistical Learning）：数学要求高，内容更广

#### 章节安排：

1. 导论与统计基础复习
2. 统计学习
3. 线性回归
4. 分类
5. 重抽样方法
6. 线性模型选择与正则化
7. 非线性模型
8. 基于树的方法
9. 支持向量机
10. 指导学习

---

## 第1章-2 统计基础复习 知识点整理

### 一、概率基础

#### 1. 随机试验 (Random Experiment)

- 满足三个条件：
  - 所有可能结果明确且不止一个
  - 结果具有不确定性（随机性）
  - 条件可重复（统计规律性）

#### 2. 基本事件与样本空间

- **基本事件 (Generic element) ω**：试验的基本结果
- **样本空间 (Sample space) Ω**：所有基本事件的集合
- 特征：
  - 完备性：每次试验必出现一个 ω ∈ Ω
  - 互斥性：每次试验只出现一个 ω
  - 最简性：基本事件不可再分

#### 3. 随机事件 (Random Event)

- 样本空间的子集 A ⊆ Ω
- 基本事件是最简单的随机事件
- 随机事件是人为关注的某些基本事件的组合

#### 4. 概率的统计定义（统计概型）

- **频数 (Occurrence)**：事件 A 在 n 次试验中出现的次数 n_A
- **频率 (Frequency)**：$f_A = n_A / n$
- 频率的性质：
  - 非负性：$f_A ≥ 0$
  - 归一性：$f_Ω = 1$
  - 可加性：若 A、B 互斥，则 $f_{A∪B} = f_A + f_B$
- **概率 (Probability)**：频率稳定于的常数，记为 P(A)
- 概率是事件固有的性质，频率是试验结果

#### 5. 随机变量 (Random Variable)

- 对随机事件的数值化度量
- **离散型**：取值有限或可列
- **连续型**：取值不可列（如时间、长度等）
- 定义：X(ω) 是样本空间 Ω 上的实值函数，且满足可测性

#### 6. 概率密度函数 (PDF) 与分布函数 (CDF)

- 对于连续型随机变量 X：
  - PDF：$p(x) ≥ 0，∫p(x)dx = 1$
  - CDF：$F(x) = P(X ≤ x) = ∫_{-∞}^x p(t)dt$
- 对于离散型随机变量：
  - 概率质量函数 (PMF)：P(X = k)

---

### 二、常用概率分布

#### 1. 离散型分布

#### (1) 二项分布 (Binomial Distribution)

- $X ~ B(n, p)$
- $P(X = k) = C(n,k) p^k (1-p)^{n-k}$
- 应用：抛硬币、有放回抽样、药物治疗效果

#### (2) 泊松分布 (Poisson Distribution)

- $X ~ Π(λ)$
- $P(X = k) = e^{-λ} λ^k / k!$
- 应用：单位时间内事件发生次数的建模

#### 2. 连续型分布

#### (1) 均匀分布 (Uniform Distribution)

- $X ~ U(a, b)$
- $p(x) = 1/(b-a) for x ∈ [a, b]$

#### (2) 指数分布 (Exponential Distribution)

- $p(x) = λe^{-λx} for x > 0$
- 应用：泊松事件流中两次事件的间隔时间

#### (3) 正态分布 (Normal Distribution)

- $X ~ N(μ, σ²)$
- $p(x) = (1/σ√(2π)) exp{-(x-μ)²/(2σ²)}$
- 应用：自然界中大量独立微小因素叠加的结果

---

### 三、随机变量的数字特征

#### 1. 均值（数学期望）

- 离散型：$E(X) = ∑ x_i p_i$
- 连续型：$E(X) = ∫ x p(x) dx$

#### 2. 方差与标准差

- 方差：$D(X) = E[(X - E(X))²]$
- 标准差：$σ = √D(X)$

---

### 四、统计学基础

#### 1. 总体与样本

- **总体 (Population)**：研究对象的全体
- **样本 (Sample)**：从总体中抽取的有代表性的子集
- **简单随机抽样 (SRS)**：满足独立同分布 (IID) 条件

#### 2. 统计量

- 定义：样本的函数 $g(x₁, x₂, ..., x_n)$
- 常用统计量：
  - 样本均值：$x̄ = (1/n)∑x_i$
  - 样本方差：$s² = (1/(n-1))∑(x_i - x̄)²$

#### 3. 统计推断

#### (1) 参数估计

- 点估计：用样本统计量估计总体参数（如 μ ≈ x̄, σ² ≈ s²）
- 方法：矩估计、最小二乘、最大似然估计

#### (2) 假设检验

- 步骤：
  - 提出原假设 H₀ 与备择假设 H₁
  - 选择显著性水平 α
  - 构造检验统计量
  - 计算 p 值
  - 做出决策：p < α → 拒绝 H₀

---

### 五、概率模型与重要公式

#### 1. 条件概率

- $P(A|B) = P(AB) / P(B)$

#### 2. 全概率公式

- 若 B₁, B₂, ..., B_n 构成完备事件组，则：
  - $P(A) = ∑ P(B_i) P(A|B_i)$

#### 3. 贝叶斯公式

- $P(B_i|A) = [P(B_i) P(A|B_i)] / ∑ P(B_j) P(A|B_j)$
- 应用：机器学习、医学诊断、垃圾邮件过滤

#### 4. 最大似然估计 (MLE)

- 选择使似然函数 P(D|θ) 最大的参数 θ

#### 5. 贝叶斯估计

- 引入先验分布 P(θ)，计算后验分布 P(θ|D)
- MAP估计：最大化后验概率

---

### 六、线性代数初步

#### 1. 向量

- n 维向量：有序数值序列
- 运算：加法、数乘、内积、叉积

#### 2. 矩阵

- 表示：M×N 矩形阵列
- 运算：加法、数乘、乘法、转置

#### 3. 线性方程组求解

- 矩阵形式：AX = B
- 解法：逆矩阵法、高斯消元法

---

### 附：本福特定律 (Benford's Law)

- 描述：自然数据中首位数字出现的概率分布
- 应用：检测数据造假（如选举、财务报表）
- 条件：数据量 ≥ 3000，非人为操控

## 第二章 统计学习

### 1. 什么是统计学习？

#### 1.1 基本假设

- 统计学习模型假设：\[ $Y = f(X) + \varepsilon$ \]
  - \( Y \)：响应变量（因变量）
  - \( X \)：预测变量（自变量，特征向量）
  - \( f \)：未知函数，表示系统信息
  - \( \varepsilon \)：随机误差项，与 \( X \) 独立，均值为 0

#### 1.2 估计 \( f \) 的目的

- **预测**：在新点 \( X = x \) 上预测 \( Y \)
- **推断**：理解哪些变量重要，如何影响 \( Y \)
- **解释性**：统计学习区别于黑盒 AI 的重要特点

#### 1.3 理想的 \( f(x) \)

- 定义：\[ $f(x) = E(Y | X = x)$ \]
- 称为**回归函数**，是使均方误差最小的函数

#### 1.4 估计方法

- **参数模型**：假设 \( f \) 的形式（如线性模型），估计参数
  - 例：线性回归
    \[ $f_L(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p$ \]
- **非参数模型**：不对 \( f \) 做假设，直接拟合数据
  - 例：薄板样条（需调节光滑参数 \( \lambda \)）
  - 需要更多数据，易过拟合

---

### 2. 模型选择与评价

#### 2.1 预测精度 vs. 可解释性

- **可解释性高**：线性模型等简单模型
- **预测精度高**：复杂模型（如 SVM、Boosting），但可能为黑盒
- **过拟合**：模型在训练集上误差小，测试集上误差大
- **欠拟合**：模型过于简单，无法捕捉数据规律

#### 2.2 拟合效果检验

- **均方误差（MSE）**：
  - 训练 MSE：\[ $MSE_{Tr} = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{f}(x_i))^2$ \]
  - 测试 MSE：
    \[ $MSE_{Te} = \frac{1}{M} \sum_{i=1}^M (y_i - \hat{f}(x_i))^2$ \]
- 测试误差更反映泛化能力

#### 2.3 偏差-方差权衡

- 测试误差可分解为：

$  E[(y_0 - \hat{f}(x_0))^2] = \text{Var}(\hat{f}(x_0)) + [\text{Bias}(\hat{f}(x_0))]^2 + \text{Var}(\varepsilon)$

- **偏差**：模型假设与真实函数的差异
- **方差**：模型对训练数据变化的敏感度
- **不可约误差**：\( \text{Var}(\varepsilon) \)，无法避免
- 随着模型复杂度增加：
  - 偏差 ↓，方差 ↑
  - 测试误差呈 U 型曲线

---

### 3. 分类问题

#### 3.1 基本概念

- 响应变量 \( Y \) 为定性变量（如垃圾邮件分类、手写数字识别）
- 目标：构建分类器 \( C(X) \)，预测类别标签

#### 3.2 评价指标

- **误分类错误率**：
 
$  Err_{Te} = \frac{1}{M} \sum_{i=1}^M I(y_i \neq \hat{C}(x_i))$
  

#### 3.3 贝叶斯分类器

- 将 \( x \) 分到条件概率最大的类：
  
  $C(x) = \arg\max_j \Pr(Y = j | X = x)$
  
- 贝叶斯错误率是最低可能错误率，但通常未知

#### 3.4 K 近邻（KNN）分类

- 估计条件概率：
  
  $\Pr(Y = j | X = x_0) = \frac{1}{K} \sum_{i \in N_0} I(y_i = j)$
  
- \( K \) 的选择影响模型复杂度：
  - \( K \) 小 → 高方差，易过拟合
  - \( K \) 大 → 高偏差，易欠拟合

---

### 4. 学习类型总结

| 类型       | 特点                  | 典型方法               |
| ---------- | --------------------- | ---------------------- |
| 监督学习   | 有\( X \) 和 \( Y \)  | 线性回归、SVM、KNN     |
| 无监督学习 | 只有\( X \)           | 聚类分析               |
| 半监督学习 | 部分有\( Y \)，部分无 | 结合有标签与无标签数据 |

---

### 5. 关键结论

- 统计学习的目标是最小化**测试误差**
- 需在**偏差与方差**之间取得平衡
- 模型复杂度（光滑度/柔性水平）需根据真实函数特性选择
- 分类问题中，贝叶斯分类器是最优但通常不可实现
- KNN 等非参数方法需谨慎选择超参数（如 \( K \)）

---

## 第三章 线性回归

## 第四章 分类

## 第五章 重抽样方法

## 第六章 线性模型选择与正则化

## 第七章 非线性模型

## 第八章 基于树的方法

## 第九章 支持向量机

## 第十章 无指导学习
